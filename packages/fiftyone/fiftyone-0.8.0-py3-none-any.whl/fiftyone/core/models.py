"""
FiftyOne models.

| Copyright 2017-2021, Voxel51, Inc.
| `voxel51.com <https://voxel51.com/>`_
|
"""
import contextlib
import itertools
import logging
import warnings

import numpy as np

import eta.core.image as etai
import eta.core.learning as etal
import eta.core.models as etam
import eta.core.utils as etau
import eta.core.video as etav
import eta.core.web as etaw

import fiftyone as fo
import fiftyone.core.labels as fol
import fiftyone.core.media as fom
import fiftyone.core.utils as fou
import fiftyone.core.validation as fov

tud = fou.lazy_import("torch.utils.data")

foe = fou.lazy_import("fiftyone.core.eta_utils")
fout = fou.lazy_import("fiftyone.utils.torch")


logger = logging.getLogger(__name__)


def apply_model(
    samples,
    model,
    label_field,
    confidence_thresh=None,
    store_logits=False,
    batch_size=None,
    num_workers=None,
):
    """Applies the given :class:`Model` to the samples in the collection.

    Args:
        samples: a :class:`fiftyone.core.collections.SampleCollection`
        model: a :class:`Model`
        label_field: the name (or prefix) of the field in which to store the
            model predictions
        confidence_thresh (None): an optional confidence threshold to apply to
            any applicable labels generated by the model
        store_logits (False): whether to store logits for the model
            predictions. This is only supported when the provided ``model`` has
            logits, ``model.has_logits == True``
        batch_size (None): an optional batch size to use. Only applicable for
            image samples
        num_workers (None): the number of workers to use when loading images.
            Only applicable for Torch models
    """
    if not isinstance(model, Model):
        raise ValueError(
            "Model must be a %s instance; found %s" % (Model, type(model))
        )

    if store_logits and not model.has_logits:
        raise ValueError(
            "The provided model does not expose logits "
            "(model.has_logits = %s)" % model.has_logits
        )

    use_data_loader = isinstance(model, TorchModelMixin)

    if num_workers is not None and not use_data_loader:
        logger.warning(
            "Ignoring `num_workers` parameter; only supported for Torch models"
        )

    with contextlib.ExitStack() as context:
        try:
            if confidence_thresh is not None:
                cthresh = confidence_thresh
            else:
                cthresh = model.config.confidence_thresh

            # pylint: disable=no-member
            context.enter_context(
                fou.SetAttributes(model.config, confidence_thresh=cthresh)
            )
        except:
            pass

        if store_logits:
            # pylint: disable=no-member
            context.enter_context(fou.SetAttributes(model, store_logits=True))

        if use_data_loader:
            # pylint: disable=no-member
            context.enter_context(fou.SetAttributes(model, preprocess=False))

        # pylint: disable=no-member
        context.enter_context(model)

        if samples.media_type == fom.VIDEO:
            return _apply_video_model(
                samples, model, label_field, confidence_thresh
            )

        batch_size = _parse_batch_size(batch_size, model, use_data_loader)

        if use_data_loader:
            return _apply_image_model_data_loader(
                samples,
                model,
                label_field,
                confidence_thresh,
                batch_size,
                num_workers,
            )

        if batch_size is not None:
            return _apply_image_model_batch(
                samples, model, label_field, confidence_thresh, batch_size
            )

        return _apply_image_model_single(
            samples, model, label_field, confidence_thresh
        )


def _apply_image_model_single(samples, model, label_field, confidence_thresh):
    with fou.ProgressBar() as pb:
        for sample in pb(samples):
            img = etai.read(sample.filepath)
            labels = model.predict(img)

            sample.add_labels(
                labels, label_field, confidence_thresh=confidence_thresh
            )


def _apply_image_model_batch(
    samples, model, label_field, confidence_thresh, batch_size
):
    samples_loader = fou.iter_batches(samples, batch_size)

    with fou.ProgressBar(samples) as pb:
        for sample_batch in samples_loader:
            imgs = [etai.read(sample.filepath) for sample in sample_batch]
            labels_batch = model.predict_all(imgs)

            for sample, labels in zip(sample_batch, labels_batch):
                sample.add_labels(
                    labels, label_field, confidence_thresh=confidence_thresh,
                )

            pb.set_iteration(pb.iteration + len(imgs))


def _apply_image_model_data_loader(
    samples, model, label_field, confidence_thresh, batch_size, num_workers
):
    samples_loader = fou.iter_batches(samples, batch_size)
    data_loader = _make_data_loader(samples, model, batch_size, num_workers)

    with fou.ProgressBar(samples) as pb:
        for sample_batch, imgs in zip(samples_loader, data_loader):
            labels_batch = model.predict_all(imgs)

            for sample, labels in zip(sample_batch, labels_batch):
                sample.add_labels(
                    labels, label_field, confidence_thresh=confidence_thresh,
                )

            pb.set_iteration(pb.iteration + len(imgs))


def _make_data_loader(samples, model, batch_size, num_workers):
    # This function supports DataLoaders that emit numpy arrays that can
    # therefore be used for non-Torch models; but we do not currenly use this
    # functionality
    use_numpy = not isinstance(model, TorchModelMixin)

    if num_workers is None:
        num_workers = fout.recommend_num_workers()

    dataset = fout.TorchImageDataset(
        samples.values("filepath"),
        transform=model.transforms,
        use_numpy=use_numpy,
        force_rgb=True,
    )

    if model.ragged_batches:
        kwargs = dict(collate_fn=lambda batch: batch)  # return list
    elif use_numpy:
        kwargs = dict(collate_fn=lambda batch: np.stack(batch, axis=0))
    else:
        kwargs = {}

    if batch_size is None:
        batch_size = 1

    return tud.DataLoader(
        dataset, batch_size=batch_size, num_workers=num_workers, **kwargs
    )


def _apply_video_model(samples, model, label_field, confidence_thresh):
    with fou.ProgressBar() as pb:
        for sample in pb(samples):
            with etav.FFmpegVideoReader(sample.filepath) as video_reader:
                labels = model.predict(video_reader)

            # Save labels
            sample.add_labels(
                labels, label_field, confidence_thresh=confidence_thresh
            )


def compute_embeddings(
    samples, model, embeddings_field=None, batch_size=None, num_workers=None,
):
    """Computes embeddings for the samples in the collection using the given
    :class:`Model`.

    The ``model`` must expose embeddings, i.e., :meth:`Model.has_embeddings`
    must return ``True``.

    If an ``embeddings_field`` is provided, the embeddings are saved to the
    samples; otherwise, the embeddings are returned in-memory.

    Args:
        samples: a :class:`fiftyone.core.collections.SampleCollection`
        model: a :class:`Model`
        embeddings_field (None): the name of a field in which to store the
            embeddings
        batch_size (None): an optional batch size to use. Only applicable for
            image samples
        num_workers (None): the number of workers to use when loading images.
            Only applicable for Torch models

    Returns:
        ``None``, if an ``embeddings_field`` is provided; otherwise, a numpy
        array whose first dimension is ``len(samples)`` containing the
        embeddings
    """
    if not isinstance(model, Model):
        raise ValueError(
            "Model must be a %s instance; found %s" % (Model, type(model))
        )

    if not model.has_embeddings:
        raise ValueError(
            "Model does not expose embeddings (model.has_embeddings = %s)"
            % model.has_embeddings
        )

    use_data_loader = isinstance(model, TorchModelMixin)

    if num_workers is not None and not use_data_loader:
        logger.warning(
            "Ignoring `num_workers` parameter; only supported for Torch models"
        )

    with contextlib.ExitStack() as context:
        if use_data_loader:
            # pylint: disable=no-member
            context.enter_context(fou.SetAttributes(model, preprocess=False))

        # pylint: disable=no-member
        context.enter_context(model)

        if samples.media_type == fom.VIDEO:
            return _compute_video_embeddings(samples, model, embeddings_field)

        batch_size = _parse_batch_size(batch_size, model, use_data_loader)

        if use_data_loader:
            return _compute_image_embeddings_data_loader(
                samples, model, embeddings_field, batch_size, num_workers
            )

        if batch_size is not None:
            return _compute_image_embeddings_batch(
                samples, model, embeddings_field, batch_size
            )

        return _compute_image_embeddings_single(
            samples, model, embeddings_field
        )


def _compute_image_embeddings_single(samples, model, embeddings_field):
    embeddings = []

    with fou.ProgressBar() as pb:
        for sample in pb(samples):
            img = etai.read(sample.filepath)
            embedding = model.embed(img)

            if embeddings_field:
                sample[embeddings_field] = embedding[0]
                sample.save()
            else:
                embeddings.append(embedding)

    if embeddings_field:
        return None

    return np.concatenate(embeddings)


def _compute_image_embeddings_batch(
    samples, model, embeddings_field, batch_size
):
    samples_loader = fou.iter_batches(samples, batch_size)

    embeddings = []

    with fou.ProgressBar(samples) as pb:
        for sample_batch in samples_loader:
            imgs = [etai.read(sample.filepath) for sample in sample_batch]
            embeddings_batch = model.embed_all(imgs)

            if embeddings_field:
                for sample, embedding in zip(sample_batch, embeddings_batch):
                    sample[embeddings_field] = embedding
                    sample.save()
            else:
                embeddings.append(embeddings_batch)

            pb.set_iteration(pb.iteration + len(imgs))

    if embeddings_field:
        return None

    return np.concatenate(embeddings)


def _compute_image_embeddings_data_loader(
    samples, model, embeddings_field, batch_size, num_workers
):
    data_loader = _make_data_loader(samples, model, batch_size, num_workers)

    if embeddings_field:
        samples_loader = fou.iter_batches(samples, batch_size)
    else:
        samples_loader = itertools.repeat(None)

    embeddings = []

    with fou.ProgressBar(samples) as pb:
        for sample_batch, imgs in zip(samples_loader, data_loader):
            embeddings_batch = model.embed_all(imgs)

            if embeddings_field:
                for sample, embedding in zip(sample_batch, embeddings_batch):
                    sample[embeddings_field] = embedding
                    sample.save()
            else:
                embeddings.append(embeddings_batch)

            pb.set_iteration(pb.iteration + len(imgs))

    if embeddings_field:
        return None

    return np.concatenate(embeddings)


def _compute_video_embeddings(samples, model, embeddings_field):
    embeddings = []

    with fou.ProgressBar() as pb:
        for sample in pb(samples):
            with etav.FFmpegVideoReader(sample.filepath) as video_reader:
                embedding = model.embed(video_reader)

            if embeddings_field:
                sample[embeddings_field] = embedding[0]
                sample.save()
            else:
                embeddings.append(embedding)

    if embeddings_field:
        return None

    return np.concatenate(embeddings)


def compute_patch_embeddings(
    samples,
    model,
    patches_field,
    embeddings_field=None,
    force_square=False,
    alpha=None,
    handle_missing="skip",
    batch_size=None,
    num_workers=None,
):
    """Computes embeddings for the image patches defined by ``patches_field``
    of the samples in the collection using the given :class:`Model`.

    The ``model`` must expose embeddings, i.e., :meth:`Model.has_embeddings`
    must return ``True``.

    If an ``embeddings_field`` is provided, the embeddings are saved to the
    samples; otherwise, the embeddings are returned in-memory.

    Args:
        samples: a :class:`fiftyone.core.collections.SampleCollection`
        model: a :class:`Model`
        patches_field: a :class:`fiftyone.core.labels.Detection`,
            :class:`fiftyone.core.labels.Detections`,
            :class:`fiftyone.core.labels.Polyline`, or
            :class:`fiftyone.core.labels.Polylines` field defining the image
            patches in each sample to embed
        embeddings_field (None): the name of a field in which to store the
            embeddings
        force_square (False): whether to minimally manipulate the patch
            bounding boxes into squares prior to extraction
        alpha (None): an optional expansion/contraction to apply to the patches
            before extracting them, in ``[-1, \infty)``. If provided, the
            length and width of the box are expanded (or contracted, when
            ``alpha < 0``) by ``(100 * alpha)%``. For example, set
            ``alpha = 1.1`` to expand the boxes by 10%, and set ``alpha = 0.9``
            to contract the boxes by 10%
        handle_missing ("skip"): how to handle images with no patches.
            Supported values are:

            -   "skip": skip the image and assign its embedding as ``None``
            -   "image": use the whole image as a single patch
            -   "error": raise an error

        batch_size (None): an optional batch size to use
        num_workers (None): the number of workers to use when loading images.
            Only applicable for Torch models

    Returns:
        ``None``, if an ``embeddings_field`` is provided; otherwise, a dict
        mapping sample IDs to arrays of patch embeddings
    """
    if samples.media_type != fom.IMAGE:
        raise ValueError("This method only supports image samples")

    if not isinstance(model, Model):
        raise ValueError(
            "Model must be a %s instance; found %s" % (Model, type(model))
        )

    if not model.has_embeddings:
        raise ValueError(
            "Model does not expose embeddings (model.has_embeddings = %s)"
            % model.has_embeddings
        )

    use_data_loader = isinstance(model, TorchModelMixin)

    _handle_missing_supported = {"skip", "image", "error"}
    if handle_missing not in _handle_missing_supported:
        raise ValueError(
            "Unsupported handle_missing = '%s'; supported values are %s"
            % (handle_missing, _handle_missing_supported)
        )

    if num_workers is not None and not use_data_loader:
        logger.warning(
            "Ignoring `num_workers` parameter; only supported for Torch models"
        )

    allowed_types = (
        fol.Detection,
        fol.Detections,
        fol.Polyline,
        fol.Polylines,
    )
    fov.validate_collection_label_fields(samples, patches_field, allowed_types)

    batch_size = _parse_batch_size(batch_size, model, use_data_loader)

    embeddings_dict = {}

    with contextlib.ExitStack() as context:
        if use_data_loader:
            # pylint: disable=no-member
            context.enter_context(fou.SetAttributes(model, preprocess=False))

        # pylint: disable=no-member
        context.enter_context(model)

        if use_data_loader:
            _embed_patches_data_loader(
                samples,
                model,
                patches_field,
                embeddings_field,
                embeddings_dict,
                force_square,
                alpha,
                handle_missing,
                batch_size,
                num_workers,
            )
        else:
            _embed_patches(
                samples,
                model,
                patches_field,
                embeddings_field,
                embeddings_dict,
                force_square,
                alpha,
                handle_missing,
                batch_size,
            )

    return embeddings_dict if not embeddings_field else None


def _embed_patches(
    samples,
    model,
    patches_field,
    embeddings_field,
    embeddings_dict,
    force_square,
    alpha,
    handle_missing,
    batch_size,
):
    with fou.ProgressBar() as pb:
        for sample in pb(samples):
            patches = _parse_patches(sample, patches_field, handle_missing)

            if patches is not None:
                img = etai.read(sample.filepath)

                if batch_size is None:
                    embeddings = _embed_patches_single(
                        model, img, patches, force_square, alpha
                    )
                else:
                    embeddings = _embed_patches_batch(
                        model, img, patches, force_square, alpha, batch_size
                    )
            else:
                embeddings = None

            if embeddings_field:
                sample[embeddings_field] = embeddings
                sample.save()
            else:
                embeddings_dict[sample.id] = embeddings


def _embed_patches_single(model, img, detections, force_square, alpha):
    embeddings = []
    for detection in detections.detections:
        patch = _extract_patch(img, detection, force_square, alpha)
        embedding = model.embed(patch)
        embeddings.append(embedding)

    return np.concatenate(embeddings)


def _embed_patches_batch(
    model, img, detections, force_square, alpha, batch_size
):
    embeddings = []
    for detection_batch in fou.iter_batches(detections.detections, batch_size):
        patches = [
            _extract_patch(img, d, force_square, alpha)
            for d in detection_batch
        ]
        embeddings_batch = model.embed_all(patches)
        embeddings.append(embeddings_batch)

    return np.concatenate(embeddings)


def _embed_patches_data_loader(
    samples,
    model,
    patches_field,
    embeddings_field,
    embeddings_dict,
    force_square,
    alpha,
    handle_missing,
    batch_size,
    num_workers,
):
    data_loader = _make_patch_data_loader(
        samples,
        model,
        patches_field,
        force_square,
        alpha,
        handle_missing,
        num_workers,
    )

    with fou.ProgressBar(samples) as pb:
        for sample, patches in pb(zip(samples, data_loader)):
            if patches is not None:
                embeddings = []
                for patches_batch in fou.iter_slices(patches, batch_size):
                    embeddings_batch = model.embed_all(patches_batch)
                    embeddings.append(embeddings_batch)

                embeddings = np.concatenate(embeddings)
            else:
                embeddings = None

            if embeddings_field:
                sample[embeddings_field] = embeddings
                sample.save()
            else:
                embeddings_dict[sample.id] = embeddings


def _make_patch_data_loader(
    samples,
    model,
    patches_field,
    force_square,
    alpha,
    handle_missing,
    num_workers,
):
    # This function supports DataLoaders that emit numpy arrays that can
    # therefore be used for non-Torch models; but we do not currenly use this
    # functionality
    use_numpy = not isinstance(model, TorchModelMixin)

    if num_workers is None:
        num_workers = fout.recommend_num_workers()

    image_paths = []
    detections = []
    for sample in samples.select_fields(patches_field):
        patches = _parse_patches(sample, patches_field, handle_missing)
        image_paths.append(sample.filepath)
        detections.append(patches)

    dataset = fout.TorchImagePatchesDataset(
        image_paths,
        detections,
        transform=model.transforms,
        ragged_batches=model.ragged_batches,
        use_numpy=use_numpy,
        force_rgb=True,
        force_square=force_square,
        alpha=alpha,
    )

    return tud.DataLoader(
        dataset,
        batch_size=1,
        num_workers=num_workers,
        collate_fn=lambda batch: batch[0],  # return patches directly
    )


def _parse_patches(sample, patches_field, handle_missing):
    label = sample[patches_field]

    if isinstance(label, fol.Detections):
        patches = label
    elif isinstance(label, fol.Detection):
        patches = fol.Detections(detections=[label])
    elif isinstance(label, fol.Polyline):
        patches = fol.Detections(detections=[label.to_detection()])
    elif isinstance(label, fol.Polylines):
        patches = label.to_detections()
    else:
        patches = None

    if patches is None or not patches.detections:
        if handle_missing == "skip":
            msg = "Sample found with no patches; embedding will be None"
            warnings.warn(msg)

            patches = None
        elif handle_missing == "image":
            msg = "Sample found with no patches; using entire image instead"
            warnings.warn(msg)

            patches = fol.Detections(
                detections=[fol.Detection(bounding_box=[0, 0, 1, 1])]
            )
        else:
            raise ValueError("Sample '%s' has no patches" % sample.id)

    return patches


def _extract_patch(img, detection, force_square, alpha):
    dobj = detection.to_detected_object()

    bbox = dobj.bounding_box
    if alpha is not None:
        bbox = bbox.pad_relative(alpha)

    return bbox.extract_from(img, force_square=force_square)


def _parse_batch_size(batch_size, model, use_data_loader):
    if batch_size is None:
        batch_size = fo.config.default_batch_size

    if batch_size is not None and batch_size > 1 and model.ragged_batches:
        logger.warning("Model does not support batching")
        batch_size = None

    if use_data_loader and batch_size is None:
        batch_size = 1

    return batch_size


def load_model(model_config_dict, model_path=None, **kwargs):
    """Loads the model specified by the given :class:`ModelConfig` dict.

    Args:
        model_config_dict: a :class:`ModelConfig` dict
        model_path (None): an optional model path to inject into the
            ``model_path`` field of the model's ``Config`` instance, which must
            implement the ``eta.core.learning.HasPublishedModel`` interface.
            This is useful when working with a model whose weights are stored
            locally and do not need to be downloaded
        **kwargs: optional keyword arguments to inject into the model's
            ``Config`` instance

    Returns:
        a :class:`Model` instance
    """
    # Inject config args
    if kwargs:
        if model_config_dict["type"] == etau.get_class_name(foe.ETAModel):
            _merge_config(model_config_dict["config"]["config"], kwargs)
        else:
            _merge_config(model_config_dict["config"], kwargs)

    # Load model config
    config = ModelConfig.from_dict(model_config_dict)

    #
    # Inject model path
    #
    # Models must be implemented in one of the following ways in order for
    # us to know how to inject ``model_path``:
    #
    # (1) Their config implements ``eta.core.learning.HasPublishedModel``
    #
    # (2) Their config is an ``fiftyone.core.eta_utils.ETAModelConfig`` whose
    #     embedded config implements ``eta.core.learning.HasPublishedModel``
    #
    if model_path:
        if isinstance(config.config, etal.HasPublishedModel):
            config.config.model_name = None
            config.config.model_path = model_path
        elif isinstance(config.config, foe.ETAModelConfig) and isinstance(
            config.config.config, etal.HasPublishedModel
        ):
            config.config.config.model_name = None
            config.config.config.model_path = model_path
        else:
            raise ValueError(
                "Model config must implement the %s interface"
                % etal.HasPublishedModel
            )

    # Build model
    return config.build()


def _merge_config(d, kwargs):
    for k, v in kwargs.items():
        if k in d and isinstance(d[k], dict):
            d[k].update(v)
        else:
            d[k] = v


class ModelConfig(etal.ModelConfig):
    """Base configuration class that encapsulates the name of a :class:`Model`
    and an instance of its associated Config class.

    Args:
        type: the fully-qualified class name of the :class:`Model` subclass
        config: an instance of the Config class associated with the model
    """

    pass


class Model(etal.Model):
    """Abstract base class for models.

    This class declares the following conventions:

    (a)     :meth:`Model.__init__` should take a single ``config`` argument
            that is an instance of ``<Model>Config``

    (b)     Models implement the context manager interface. This means that
            models can optionally use context to perform any necessary setup
            and teardown, and so any code that builds a model should use the
            ``with`` syntax
    """

    def __enter__(self):
        return self

    def __exit__(self, *args):
        pass

    @property
    def has_logits(self):
        """Whether this instance can generate logits for its predictions.

        This method returns ``False`` by default. Methods that can generate
        logits will override this via implementing the
        :class:`LogitsMixin` interface.
        """
        return False

    @property
    def has_embeddings(self):
        """Whether this instance can generate embeddings.

        This method returns ``False`` by default. Methods that can generate
        embeddings will override this via implementing the
        :class:`EmbeddingsMixin` interface.
        """
        return False

    @property
    def ragged_batches(self):
        """True/False whether :meth:`transforms` may return tensors of
        different sizes. If True, then passing ragged lists of data to
        :meth:`predict_all` is not allowed.
        """
        raise NotImplementedError("subclasses must implement ragged_batches")

    @property
    def transforms(self):
        """The preprocessing function that will/must be applied to each input
        before prediction, or ``None`` if no preprocessing is performed.
        """
        raise NotImplementedError("subclasses must implement transforms")

    @property
    def preprocess(self):
        """Whether to apply :meth:`transforms` during inference (True) or to
        assume that they have already been applied (False).
        """
        raise NotImplementedError("subclasses must implement preprocess")

    @preprocess.setter
    def preprocess(self, value):
        raise NotImplementedError("subclasses must implement preprocess")

    def predict(self, arg):
        """Peforms prediction on the given data.

        Image models should support, at minimum, processing ``arg`` values that
        are uint8 numpy arrays (HWC).

        Video models should support, at minimum, processing ``arg`` values that
        are ``eta.core.video.VideoReader`` instances.

        Args:
            arg: the data

        Returns:
            a :class:`fiftyone.core.labels.Label` instance or dict of
            :class:`fiftyone.core.labels.Label` instances containing the
            predictions
        """
        raise NotImplementedError("subclasses must implement predict()")

    def predict_all(self, args):
        """Performs prediction on the given iterable of data.

        Image models should support, at minimum, processing ``args`` values
        that are either lists of uint8 numpy arrays (HWC) or numpy array
        tensors (NHWC).

        Video models should support, at minimum, processing ``args`` values
        that are lists of ``eta.core.video.VideoReader`` instances.

        Subclasses can override this method to increase efficiency, but, by
        default, this method simply iterates over the data and applies
        :meth:`predict` to each.

        Args:
            args: an iterable of data

        Returns:
            a list of :class:`fiftyone.core.labels.Label` instances or a list
            of dicts of :class:`fiftyone.core.labels.Label` instances
            containing the predictions
        """
        return [self.predict(arg) for arg in args]


class LogitsMixin(object):
    """Mixin for :class:`Model` classes that can generate logits for their
    predictions.

    This mixin allows for the possibility that only some instances of a class
    are capable of generating logits, per the value of the
    :meth:`has_logits` property.
    """

    def __init__(self):
        self._store_logits = False

    @property
    def store_logits(self):
        """Whether the model should store logits in its predictions."""
        return self._store_logits

    @store_logits.setter
    def store_logits(self, flag):
        if flag and not self.has_logits:
            raise ValueError("This model cannot generate logits to store")

        self._store_logits = flag

    @property
    def has_logits(self):
        """Whether this instance can generate logits."""
        raise NotImplementedError("subclasses must implement has_logits")


class EmbeddingsMixin(object):
    """Mixin for :class:`Model` classes that can generate embeddings for
    their predictions.

    This mixin allows for the possibility that only some instances of a class
    are capable of generating embeddings, per the value of the
    :meth:`has_embeddings` property.
    """

    @property
    def has_embeddings(self):
        """Whether this instance has embeddings."""
        raise NotImplementedError("subclasses must implement has_embeddings")

    def get_embeddings(self):
        """Returns the embeddings generated by the last forward pass of the
        model.

        By convention, this method should always return an array whose first
        axis represents batch size (which will always be 1 when :meth:`predict`
        was last used).

        Returns:
            a numpy array containing the embedding(s)
        """
        raise NotImplementedError("subclasses must implement get_embeddings()")

    def embed(self, arg):
        """Generates an embedding for the given data.

        Subclasses can override this method to increase efficiency, but, by
        default, this method simply calls :meth:`predict` and then returns
        :meth:`get_embeddings`.

        Args:
            arg: the data. See :meth:`predict` for details

        Returns:
            a numpy array containing the embedding
        """
        # pylint: disable=no-member
        self.predict(arg)
        return self.get_embeddings()

    def embed_all(self, args):
        """Generates embeddings for the given iterable of data.

        Subclasses can override this method to increase efficiency, but, by
        default, this method simply iterates over the data and applies
        :meth:`embed` to each.

        Args:
            args: an iterable of data. See :meth:`predict_all` for details

        Returns:
            a numpy array containing the embeddings stacked along axis 0
        """
        return np.stack([self.embed(arg) for arg in args], axis=0)


class TorchModelMixin(object):
    """Mixin for :class:`Model` classes that support feeding data for inference
    via a ``torch.utils.data.DataLoader``.

    Models implementing this mixin must expose via their
    :meth:`Model.transforms` property the ``torchvision.transforms`` function
    that will/must be applied to each input before prediction.
    """

    pass


class ModelManagerConfig(etam.ModelManagerConfig):
    """Config settings for a :class:`ModelManager`.

    Args:
        url (None): the URL of the file
        google_drive_id (None): the ID of the file in Google Drive
        extract_archive (None): whether to extract the downloaded model, which
            is assumed to be an archive
        delete_archive (None): whether to delete the archive after extracting
            it, if applicable
    """

    def __init__(self, d):
        super().__init__(d)

        self.url = self.parse_string(d, "url", default=None)
        self.google_drive_id = self.parse_string(
            d, "google_drive_id", default=None
        )


class ModelManager(etam.ModelManager):
    """Class for downloading FiftyOne models from the web."""

    @staticmethod
    def upload_model(model_path, *args, **kwargs):
        raise NotImplementedError("Uploading models via API is not supported")

    def _download_model(self, model_path):
        if self.config.google_drive_id:
            gid = self.config.google_drive_id
            logger.info("Downloading model from Google Drive ID '%s'...", gid)
            etaw.download_google_drive_file(gid, path=model_path)
        elif self.config.url:
            url = self.config.url
            logger.info("Downloading model from '%s'...", url)
            etaw.download_file(url, path=model_path)
        else:
            raise ValueError(
                "Invalid ModelManagerConfig '%s'" % str(self.config)
            )

    def delete_model(self):
        raise NotImplementedError("Deleting models via API is not supported")
