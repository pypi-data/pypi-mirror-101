-
  Q: My jupyter kernel keeps crashing. 
  A: You need to monitor you memory usage and make sure it isn't swapping.
-
  Q: Is swap when you are creating variables?
  A: No. Swap is when you overflow the RAM (volatile memory) and it has to create variables and objects on disk (persistent memory). Disk writing and reading is 100x or 1000x slower than reading and writing from RAM. And if you CPU gets bogged down with multiple processes reading and writing from disk it can freeze or crash the kernel.
-
  Q: How can I reduce my memory (RAM) usage?
  A: "First you have to find out which lines of code are using the most memory. For that you can run `htop` in a terminal window or tab to the side of your screen. on mac: `brew install htop`. On linux `sudo -apt-get install htop`"
-
  Q: How can I monitor my memory (RAM) usage?
  A: "Run `htop` in a terminal window or tab to the side of your screen as you run your code a few lines at a time. If you need more detailed information about which functions and "
-
  Q: Which lines of python code are using up the most RAM?
  A: Focus your attention on objects that contain training or testsets for machine learning, especially processed images or natural language. Use an external tool like `htop` to monitor CPU and memory usage. Some python tools and packages can also measure memory usage. The builtin `sys` package contains the `getsizeof()` function which can tell you the approximate number of bytes used by any python object. Also, this [pluralsight tutorial](https://www.pluralsight.com/blog/tutorials/how-to-profile-memory-usage-in-python) suggests a tools called `hpy` within guppy (pip install guppy). It displays all the objects in your heap and their memory usage. 
-
  Q: In a job interview for a data science position they are asking me business questions about how I can determine the objective or problem statement for a data science project.
  A: Explain to the itnerviewer that you would need to interview the business stakeholders (managers) to find out what their objectives are and see if there are ways to measure these variables in existing datasets that are collected regularly. Unfortunately, that will usually bias the model towards short term business success (profit and revenue), because that's what most business managers care about the most. It can help to try to build a predict variables like profit and revenue a year in advance. This will ensure you are optimizing for long term business success. It would be even better if you could interview users (the forgotten stakeholder) rather than business managers. You want to find out how they interact with your system when they are extremely satisfied or dissatisfied with your product. You will also need to watch out for confounding variables or spurious correlations. You can check for the dosage effect -- the error (residuals) should be uniformly distributed around your predictions, irrespective of the magnitude of the predicted variable. The accuracy or power of the machine learning model for the predictive variables should be high. Using Lasso or Ridge or other machine learning model regularization techniques can help ensure this. Normalizing for as many variables as possible by including them as machine learning features in your training dataset can help. Engineering many features based on the residual plots can also help ensure that confounding variables have been normalized away. Also, there needs to be a theoretical or physical reason for why the business managers sustpect that the dominant feature variables could cause the target variable to change. In the end, the only way to be sure the effect is causal, is to perform an experiment, like an A/B test or a randomized controled trial (preferably double-blind). 
